{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe850d6",
   "metadata": {},
   "source": [
    "## 1. Setup - Upload Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục project\n",
    "!mkdir -p AutoencoderCpu/src\n",
    "%cd AutoencoderCpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0eef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/common.h\n",
    "/**\n",
    " * @file common.h\n",
    " * @brief Common definitions and includes for the Autoencoder project\n",
    " */\n",
    "\n",
    "#ifndef COMMON_H\n",
    "#define COMMON_H\n",
    "\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <cstring>\n",
    "#include <fstream>\n",
    "#include <random>\n",
    "#include <chrono>\n",
    "#include <iomanip>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <cassert>\n",
    "#include <map>\n",
    "\n",
    "using DataType = float;\n",
    "\n",
    "class Tensor4D {\n",
    "public:\n",
    "    int batch, height, width, channels;\n",
    "    std::vector<DataType> data;\n",
    "\n",
    "    Tensor4D() : batch(0), height(0), width(0), channels(0) {}\n",
    "    Tensor4D(int n, int h, int w, int c) : batch(n), height(h), width(w), channels(c) {\n",
    "        data.resize(n * h * w * c, 0.0f);\n",
    "    }\n",
    "\n",
    "    size_t size() const { return batch * height * width * channels; }\n",
    "    size_t memorySize() const { return size() * sizeof(DataType); }\n",
    "\n",
    "    DataType& at(int n, int h, int w, int c) {\n",
    "        return data[((n * height + h) * width + w) * channels + c];\n",
    "    }\n",
    "    const DataType& at(int n, int h, int w, int c) const {\n",
    "        return data[((n * height + h) * width + w) * channels + c];\n",
    "    }\n",
    "\n",
    "    void fill(DataType value) { std::fill(data.begin(), data.end(), value); }\n",
    "    \n",
    "    void randomInit(float scale = 1.0f) {\n",
    "        std::random_device rd;\n",
    "        std::mt19937 gen(rd());\n",
    "        std::normal_distribution<float> dist(0.0f, scale);\n",
    "        for (auto& val : data) val = dist(gen);\n",
    "    }\n",
    "\n",
    "    void copyFrom(const Tensor4D& other) {\n",
    "        batch = other.batch; height = other.height;\n",
    "        width = other.width; channels = other.channels;\n",
    "        data = other.data;\n",
    "    }\n",
    "};\n",
    "\n",
    "class Timer {\n",
    "private:\n",
    "    std::chrono::high_resolution_clock::time_point startTime, endTime;\n",
    "    bool running;\n",
    "public:\n",
    "    Timer() : running(false) {}\n",
    "    void start() { startTime = std::chrono::high_resolution_clock::now(); running = true; }\n",
    "    void stop() { endTime = std::chrono::high_resolution_clock::now(); running = false; }\n",
    "    double elapsedMs() const {\n",
    "        auto end = running ? std::chrono::high_resolution_clock::now() : endTime;\n",
    "        return std::chrono::duration<double, std::milli>(end - startTime).count();\n",
    "    }\n",
    "    double elapsedSec() const { return elapsedMs() / 1000.0; }\n",
    "};\n",
    "\n",
    "class Profiler {\n",
    "public:\n",
    "    struct Stats { double totalTime = 0; int callCount = 0; double avgTime() const { return callCount > 0 ? totalTime / callCount : 0; } };\n",
    "    std::map<std::string, Stats> stats;\n",
    "    void addTime(const std::string& name, double timeMs) { stats[name].totalTime += timeMs; stats[name].callCount++; }\n",
    "    void reset() { stats.clear(); }\n",
    "    void printReport() const {\n",
    "        std::cout << \"\\n========== PROFILER REPORT ==========\\n\";\n",
    "        double total = 0;\n",
    "        for (const auto& [name, stat] : stats) total += stat.totalTime;\n",
    "        std::vector<std::pair<std::string, Stats>> sorted(stats.begin(), stats.end());\n",
    "        std::sort(sorted.begin(), sorted.end(), [](const auto& a, const auto& b) { return a.second.totalTime > b.second.totalTime; });\n",
    "        std::cout << std::left << std::setw(20) << \"Operation\" << std::right << std::setw(15) << \"Total (ms)\" << std::setw(10) << \"Calls\" << std::setw(15) << \"Percentage\" << \"\\n\";\n",
    "        for (const auto& [name, stat] : sorted) {\n",
    "            double pct = total > 0 ? stat.totalTime / total * 100 : 0;\n",
    "            std::cout << std::left << std::setw(20) << name << std::right << std::setw(15) << std::fixed << std::setprecision(2) << stat.totalTime << std::setw(10) << stat.callCount << std::setw(14) << pct << \"%\\n\";\n",
    "        }\n",
    "        std::cout << \"======================================\\n\";\n",
    "    }\n",
    "};\n",
    "\n",
    "class MemoryTracker {\n",
    "public:\n",
    "    size_t weightsMemory = 0, activationsMemory = 0, gradientsMemory = 0, dataMemory = 0;\n",
    "    void addWeights(size_t bytes) { weightsMemory += bytes; }\n",
    "    void addActivations(size_t bytes) { activationsMemory += bytes; }\n",
    "    void addGradients(size_t bytes) { gradientsMemory += bytes; }\n",
    "    void addData(size_t bytes) { dataMemory += bytes; }\n",
    "    size_t totalMemory() const { return weightsMemory + activationsMemory + gradientsMemory + dataMemory; }\n",
    "    void printReport() const {\n",
    "        std::cout << \"\\n========== MEMORY USAGE ==========\\n\";\n",
    "        std::cout << \"Weights:     \" << (weightsMemory / 1024.0 / 1024.0) << \" MB\\n\";\n",
    "        std::cout << \"Activations: \" << (activationsMemory / 1024.0 / 1024.0) << \" MB\\n\";\n",
    "        std::cout << \"Gradients:   \" << (gradientsMemory / 1024.0 / 1024.0) << \" MB\\n\";\n",
    "        std::cout << \"Data:        \" << (dataMemory / 1024.0 / 1024.0) << \" MB\\n\";\n",
    "        std::cout << \"TOTAL:       \" << (totalMemory() / 1024.0 / 1024.0) << \" MB\\n\";\n",
    "        std::cout << \"==================================\\n\";\n",
    "    }\n",
    "};\n",
    "\n",
    "extern Profiler gProfiler;\n",
    "extern MemoryTracker gMemoryTracker;\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a466c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/cifar10.h\n",
    "#ifndef CIFAR10_H\n",
    "#define CIFAR10_H\n",
    "\n",
    "#include \"common.h\"\n",
    "#include <string>\n",
    "\n",
    "class CIFAR10Dataset {\n",
    "public:\n",
    "    static constexpr int IMAGE_HEIGHT = 32, IMAGE_WIDTH = 32, IMAGE_CHANNELS = 3;\n",
    "    static constexpr int IMAGE_SIZE = 3072, NUM_CLASSES = 10, IMAGES_PER_BATCH = 10000;\n",
    "    static constexpr int NUM_TRAIN_BATCHES = 5, TOTAL_TRAIN_IMAGES = 50000, TOTAL_TEST_IMAGES = 10000;\n",
    "\n",
    "private:\n",
    "    std::vector<DataType> trainImages, testImages;\n",
    "    std::vector<int> trainLabels, testLabels;\n",
    "    bool loaded = false;\n",
    "    int numTrainImages = 0, numTestImages = 0;\n",
    "\n",
    "public:\n",
    "    bool load(const std::string& path, int maxTrainSamples = 0, float testRatio = 0.2f) {\n",
    "        int targetTrain = (maxTrainSamples > 0 && maxTrainSamples < TOTAL_TRAIN_IMAGES) ? maxTrainSamples : TOTAL_TRAIN_IMAGES;\n",
    "        int targetTest = (maxTrainSamples > 0) ? std::max(1, (int)(targetTrain * testRatio)) : TOTAL_TEST_IMAGES;\n",
    "        targetTest = std::min(targetTest, TOTAL_TEST_IMAGES);\n",
    "        \n",
    "        std::cout << \"Loading CIFAR-10 from: \" << path << std::endl;\n",
    "        if (maxTrainSamples > 0) std::cout << \"  [LIMITED] Train: \" << targetTrain << \", Test: \" << targetTest << std::endl;\n",
    "\n",
    "        trainImages.resize(targetTrain * IMAGE_SIZE);\n",
    "        trainLabels.resize(targetTrain);\n",
    "        \n",
    "        int loadedTrain = 0;\n",
    "        for (int batch = 1; batch <= NUM_TRAIN_BATCHES && loadedTrain < targetTrain; batch++) {\n",
    "            std::string filename = path + \"/data_batch_\" + std::to_string(batch) + \".bin\";\n",
    "            int toLoad = std::min(IMAGES_PER_BATCH, targetTrain - loadedTrain);\n",
    "            if (!loadBatch(filename, trainImages.data() + loadedTrain * IMAGE_SIZE, trainLabels.data() + loadedTrain, toLoad)) return false;\n",
    "            loadedTrain += toLoad;\n",
    "            std::cout << \"  Loaded batch \" << batch << \" (\" << toLoad << \" images)\" << std::endl;\n",
    "        }\n",
    "        numTrainImages = loadedTrain;\n",
    "\n",
    "        testImages.resize(targetTest * IMAGE_SIZE);\n",
    "        testLabels.resize(targetTest);\n",
    "        if (!loadBatch(path + \"/test_batch.bin\", testImages.data(), testLabels.data(), targetTest)) return false;\n",
    "        numTestImages = targetTest;\n",
    "        \n",
    "        loaded = true;\n",
    "        gMemoryTracker.addData((trainImages.size() + testImages.size()) * sizeof(DataType));\n",
    "        std::cout << \"Dataset loaded! Train: \" << numTrainImages << \", Test: \" << numTestImages << std::endl;\n",
    "        return true;\n",
    "    }\n",
    "\n",
    "    Tensor4D getTrainBatch(int startIdx, int batchSize) const {\n",
    "        Tensor4D batch(batchSize, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS);\n",
    "        for (int i = 0; i < batchSize; i++) {\n",
    "            const DataType* src = trainImages.data() + (startIdx + i) * IMAGE_SIZE;\n",
    "            for (int h = 0; h < IMAGE_HEIGHT; h++)\n",
    "                for (int w = 0; w < IMAGE_WIDTH; w++)\n",
    "                    for (int c = 0; c < IMAGE_CHANNELS; c++)\n",
    "                        batch.at(i, h, w, c) = src[(c * IMAGE_HEIGHT + h) * IMAGE_WIDTH + w];\n",
    "        }\n",
    "        return batch;\n",
    "    }\n",
    "\n",
    "    void shuffleTrainData() {\n",
    "        std::vector<int> idx(numTrainImages);\n",
    "        std::iota(idx.begin(), idx.end(), 0);\n",
    "        std::random_device rd; std::mt19937 gen(rd());\n",
    "        std::shuffle(idx.begin(), idx.end(), gen);\n",
    "        std::vector<DataType> tmpImg(trainImages.size());\n",
    "        std::vector<int> tmpLbl(trainLabels.size());\n",
    "        for (int i = 0; i < numTrainImages; i++) {\n",
    "            std::copy(trainImages.begin() + idx[i] * IMAGE_SIZE, trainImages.begin() + (idx[i] + 1) * IMAGE_SIZE, tmpImg.begin() + i * IMAGE_SIZE);\n",
    "            tmpLbl[i] = trainLabels[idx[i]];\n",
    "        }\n",
    "        trainImages = std::move(tmpImg); trainLabels = std::move(tmpLbl);\n",
    "    }\n",
    "\n",
    "    int getNumTrainImages() const { return numTrainImages; }\n",
    "    int getNumTestImages() const { return numTestImages; }\n",
    "\n",
    "private:\n",
    "    bool loadBatch(const std::string& filename, DataType* images, int* labels, int numImages) {\n",
    "        std::ifstream file(filename, std::ios::binary);\n",
    "        if (!file) { std::cerr << \"Cannot open: \" << filename << std::endl; return false; }\n",
    "        std::vector<unsigned char> buffer(1 + IMAGE_SIZE);\n",
    "        for (int i = 0; i < numImages; i++) {\n",
    "            file.read(reinterpret_cast<char*>(buffer.data()), buffer.size());\n",
    "            labels[i] = buffer[0];\n",
    "            for (int j = 0; j < IMAGE_SIZE; j++) images[i * IMAGE_SIZE + j] = buffer[1 + j] / 255.0f;\n",
    "        }\n",
    "        return true;\n",
    "    }\n",
    "};\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/optimizer.h\n",
    "#ifndef OPTIMIZER_H\n",
    "#define OPTIMIZER_H\n",
    "\n",
    "#include \"common.h\"\n",
    "#include <cmath>\n",
    "\n",
    "class AdamOptimizer {\n",
    "public:\n",
    "    float learningRate, beta1, beta2, epsilon;\n",
    "    int timestep = 0;\n",
    "    std::map<int, std::vector<DataType>> m, v;\n",
    "\n",
    "    AdamOptimizer(float lr = 0.001f, float b1 = 0.9f, float b2 = 0.999f, float eps = 1e-8f)\n",
    "        : learningRate(lr), beta1(b1), beta2(b2), epsilon(eps) {}\n",
    "\n",
    "    void update(std::vector<DataType>& weights, const std::vector<DataType>& gradients, int paramId) {\n",
    "        if (m.find(paramId) == m.end()) {\n",
    "            m[paramId].resize(weights.size(), 0.0f);\n",
    "            v[paramId].resize(weights.size(), 0.0f);\n",
    "        }\n",
    "        auto& m_t = m[paramId]; auto& v_t = v[paramId];\n",
    "        float bc1 = 1.0f - std::pow(beta1, timestep);\n",
    "        float bc2 = 1.0f - std::pow(beta2, timestep);\n",
    "        for (size_t i = 0; i < weights.size(); i++) {\n",
    "            m_t[i] = beta1 * m_t[i] + (1 - beta1) * gradients[i];\n",
    "            v_t[i] = beta2 * v_t[i] + (1 - beta2) * gradients[i] * gradients[i];\n",
    "            weights[i] -= learningRate * (m_t[i] / bc1) / (std::sqrt(v_t[i] / bc2) + epsilon);\n",
    "        }\n",
    "    }\n",
    "    void step() { timestep++; }\n",
    "    void reset() { timestep = 0; m.clear(); v.clear(); }\n",
    "};\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/layers_cpu.h\n",
    "#ifndef LAYERS_CPU_H\n",
    "#define LAYERS_CPU_H\n",
    "\n",
    "#include \"common.h\"\n",
    "#include <cmath>\n",
    "#include <limits>\n",
    "\n",
    "class Conv2D {\n",
    "public:\n",
    "    int inChannels, outChannels, kernelSize, padding, stride;\n",
    "    Tensor4D weights, gradWeights, inputCache;\n",
    "    std::vector<DataType> bias, gradBias;\n",
    "\n",
    "    Conv2D(int inCh, int outCh, int kSize = 3, int pad = 1, int str = 1)\n",
    "        : inChannels(inCh), outChannels(outCh), kernelSize(kSize), padding(pad), stride(str) {\n",
    "        weights = Tensor4D(kernelSize, kernelSize, inChannels, outChannels);\n",
    "        weights.randomInit(std::sqrt(2.0f / (kernelSize * kernelSize * inChannels)));\n",
    "        bias.resize(outChannels, 0.0f);\n",
    "        gradWeights = Tensor4D(kernelSize, kernelSize, inChannels, outChannels);\n",
    "        gradBias.resize(outChannels, 0.0f);\n",
    "        gMemoryTracker.addWeights(weights.memorySize() + bias.size() * sizeof(DataType));\n",
    "        gMemoryTracker.addGradients(gradWeights.memorySize() + gradBias.size() * sizeof(DataType));\n",
    "    }\n",
    "\n",
    "    Tensor4D forward(const Tensor4D& input) {\n",
    "        Timer timer; timer.start();\n",
    "        inputCache.copyFrom(input);\n",
    "        int N = input.batch, H_in = input.height, W_in = input.width;\n",
    "        int H_out = (H_in + 2*padding - kernelSize) / stride + 1;\n",
    "        int W_out = (W_in + 2*padding - kernelSize) / stride + 1;\n",
    "        Tensor4D output(N, H_out, W_out, outChannels);\n",
    "        gMemoryTracker.addActivations(output.memorySize());\n",
    "\n",
    "        for (int n = 0; n < N; n++)\n",
    "            for (int oh = 0; oh < H_out; oh++)\n",
    "                for (int ow = 0; ow < W_out; ow++)\n",
    "                    for (int oc = 0; oc < outChannels; oc++) {\n",
    "                        DataType sum = bias[oc];\n",
    "                        for (int kh = 0; kh < kernelSize; kh++)\n",
    "                            for (int kw = 0; kw < kernelSize; kw++) {\n",
    "                                int ih = oh*stride - padding + kh, iw = ow*stride - padding + kw;\n",
    "                                if (ih >= 0 && ih < H_in && iw >= 0 && iw < W_in)\n",
    "                                    for (int ic = 0; ic < inChannels; ic++)\n",
    "                                        sum += input.at(n,ih,iw,ic) * weights.at(kh,kw,ic,oc);\n",
    "                            }\n",
    "                        output.at(n,oh,ow,oc) = sum;\n",
    "                    }\n",
    "        timer.stop(); gProfiler.addTime(\"Conv2D_forward\", timer.elapsedMs());\n",
    "        return output;\n",
    "    }\n",
    "\n",
    "    Tensor4D backward(const Tensor4D& gradOutput) {\n",
    "        Timer timer; timer.start();\n",
    "        int N = inputCache.batch, H_in = inputCache.height, W_in = inputCache.width;\n",
    "        int H_out = gradOutput.height, W_out = gradOutput.width;\n",
    "        Tensor4D gradInput(N, H_in, W_in, inChannels);\n",
    "        gradWeights.fill(0); std::fill(gradBias.begin(), gradBias.end(), 0);\n",
    "\n",
    "        for (int n = 0; n < N; n++)\n",
    "            for (int oh = 0; oh < H_out; oh++)\n",
    "                for (int ow = 0; ow < W_out; ow++)\n",
    "                    for (int oc = 0; oc < outChannels; oc++) {\n",
    "                        DataType grad = gradOutput.at(n,oh,ow,oc);\n",
    "                        gradBias[oc] += grad;\n",
    "                        for (int kh = 0; kh < kernelSize; kh++)\n",
    "                            for (int kw = 0; kw < kernelSize; kw++) {\n",
    "                                int ih = oh*stride - padding + kh, iw = ow*stride - padding + kw;\n",
    "                                if (ih >= 0 && ih < H_in && iw >= 0 && iw < W_in)\n",
    "                                    for (int ic = 0; ic < inChannels; ic++) {\n",
    "                                        gradWeights.at(kh,kw,ic,oc) += inputCache.at(n,ih,iw,ic) * grad;\n",
    "                                        gradInput.at(n,ih,iw,ic) += weights.at(kh,kw,ic,oc) * grad;\n",
    "                                    }\n",
    "                            }\n",
    "                    }\n",
    "        timer.stop(); gProfiler.addTime(\"Conv2D_backward\", timer.elapsedMs());\n",
    "        return gradInput;\n",
    "    }\n",
    "\n",
    "    void updateWeights(float lr) {\n",
    "        for (size_t i = 0; i < weights.data.size(); i++) weights.data[i] -= lr * gradWeights.data[i];\n",
    "        for (size_t i = 0; i < bias.size(); i++) bias[i] -= lr * gradBias[i];\n",
    "    }\n",
    "};\n",
    "\n",
    "class ReLU {\n",
    "public:\n",
    "    Tensor4D maskCache;\n",
    "    Tensor4D forward(const Tensor4D& input) {\n",
    "        Timer timer; timer.start();\n",
    "        Tensor4D output(input.batch, input.height, input.width, input.channels);\n",
    "        maskCache = Tensor4D(input.batch, input.height, input.width, input.channels);\n",
    "        for (size_t i = 0; i < input.data.size(); i++) {\n",
    "            output.data[i] = input.data[i] > 0 ? input.data[i] : 0;\n",
    "            maskCache.data[i] = input.data[i] > 0 ? 1.0f : 0.0f;\n",
    "        }\n",
    "        gMemoryTracker.addActivations(output.memorySize());\n",
    "        timer.stop(); gProfiler.addTime(\"ReLU_forward\", timer.elapsedMs());\n",
    "        return output;\n",
    "    }\n",
    "    Tensor4D backward(const Tensor4D& gradOutput) {\n",
    "        Timer timer; timer.start();\n",
    "        Tensor4D gradInput(gradOutput.batch, gradOutput.height, gradOutput.width, gradOutput.channels);\n",
    "        for (size_t i = 0; i < gradOutput.data.size(); i++) gradInput.data[i] = gradOutput.data[i] * maskCache.data[i];\n",
    "        timer.stop(); gProfiler.addTime(\"ReLU_backward\", timer.elapsedMs());\n",
    "        return gradInput;\n",
    "    }\n",
    "};\n",
    "\n",
    "class MaxPool2D {\n",
    "public:\n",
    "    int poolSize, stride, cachedH_in, cachedW_in;\n",
    "    Tensor4D maxIndicesH, maxIndicesW;\n",
    "    MaxPool2D(int size = 2, int str = 2) : poolSize(size), stride(str) {}\n",
    "\n",
    "    Tensor4D forward(const Tensor4D& input) {\n",
    "        Timer timer; timer.start();\n",
    "        int N = input.batch, H_in = input.height, W_in = input.width, C = input.channels;\n",
    "        cachedH_in = H_in; cachedW_in = W_in;\n",
    "        int H_out = (H_in - poolSize) / stride + 1, W_out = (W_in - poolSize) / stride + 1;\n",
    "        Tensor4D output(N, H_out, W_out, C);\n",
    "        maxIndicesH = Tensor4D(N, H_out, W_out, C); maxIndicesW = Tensor4D(N, H_out, W_out, C);\n",
    "        gMemoryTracker.addActivations(output.memorySize());\n",
    "\n",
    "        for (int n = 0; n < N; n++)\n",
    "            for (int oh = 0; oh < H_out; oh++)\n",
    "                for (int ow = 0; ow < W_out; ow++)\n",
    "                    for (int c = 0; c < C; c++) {\n",
    "                        DataType maxVal = -std::numeric_limits<DataType>::infinity();\n",
    "                        int maxH = 0, maxW = 0;\n",
    "                        for (int ph = 0; ph < poolSize; ph++)\n",
    "                            for (int pw = 0; pw < poolSize; pw++) {\n",
    "                                int ih = oh*stride + ph, iw = ow*stride + pw;\n",
    "                                if (input.at(n,ih,iw,c) > maxVal) { maxVal = input.at(n,ih,iw,c); maxH = ih; maxW = iw; }\n",
    "                            }\n",
    "                        output.at(n,oh,ow,c) = maxVal;\n",
    "                        maxIndicesH.at(n,oh,ow,c) = maxH; maxIndicesW.at(n,oh,ow,c) = maxW;\n",
    "                    }\n",
    "        timer.stop(); gProfiler.addTime(\"MaxPool2D_forward\", timer.elapsedMs());\n",
    "        return output;\n",
    "    }\n",
    "\n",
    "    Tensor4D backward(const Tensor4D& gradOutput) {\n",
    "        Timer timer; timer.start();\n",
    "        Tensor4D gradInput(gradOutput.batch, cachedH_in, cachedW_in, gradOutput.channels);\n",
    "        gradInput.fill(0);\n",
    "        for (int n = 0; n < gradOutput.batch; n++)\n",
    "            for (int oh = 0; oh < gradOutput.height; oh++)\n",
    "                for (int ow = 0; ow < gradOutput.width; ow++)\n",
    "                    for (int c = 0; c < gradOutput.channels; c++) {\n",
    "                        int mh = (int)maxIndicesH.at(n,oh,ow,c), mw = (int)maxIndicesW.at(n,oh,ow,c);\n",
    "                        gradInput.at(n,mh,mw,c) += gradOutput.at(n,oh,ow,c);\n",
    "                    }\n",
    "        timer.stop(); gProfiler.addTime(\"MaxPool2D_backward\", timer.elapsedMs());\n",
    "        return gradInput;\n",
    "    }\n",
    "};\n",
    "\n",
    "class UpSample2D {\n",
    "public:\n",
    "    int scaleFactor, cachedH_in, cachedW_in;\n",
    "    UpSample2D(int scale = 2) : scaleFactor(scale) {}\n",
    "\n",
    "    Tensor4D forward(const Tensor4D& input) {\n",
    "        Timer timer; timer.start();\n",
    "        cachedH_in = input.height; cachedW_in = input.width;\n",
    "        int H_out = input.height * scaleFactor, W_out = input.width * scaleFactor;\n",
    "        Tensor4D output(input.batch, H_out, W_out, input.channels);\n",
    "        gMemoryTracker.addActivations(output.memorySize());\n",
    "        for (int n = 0; n < input.batch; n++)\n",
    "            for (int oh = 0; oh < H_out; oh++)\n",
    "                for (int ow = 0; ow < W_out; ow++)\n",
    "                    for (int c = 0; c < input.channels; c++)\n",
    "                        output.at(n,oh,ow,c) = input.at(n, oh/scaleFactor, ow/scaleFactor, c);\n",
    "        timer.stop(); gProfiler.addTime(\"UpSample2D_forward\", timer.elapsedMs());\n",
    "        return output;\n",
    "    }\n",
    "\n",
    "    Tensor4D backward(const Tensor4D& gradOutput) {\n",
    "        Timer timer; timer.start();\n",
    "        Tensor4D gradInput(gradOutput.batch, cachedH_in, cachedW_in, gradOutput.channels);\n",
    "        gradInput.fill(0);\n",
    "        for (int n = 0; n < gradOutput.batch; n++)\n",
    "            for (int oh = 0; oh < gradOutput.height; oh++)\n",
    "                for (int ow = 0; ow < gradOutput.width; ow++)\n",
    "                    for (int c = 0; c < gradOutput.channels; c++)\n",
    "                        gradInput.at(n, oh/scaleFactor, ow/scaleFactor, c) += gradOutput.at(n,oh,ow,c);\n",
    "        timer.stop(); gProfiler.addTime(\"UpSample2D_backward\", timer.elapsedMs());\n",
    "        return gradInput;\n",
    "    }\n",
    "};\n",
    "\n",
    "class MSELoss {\n",
    "public:\n",
    "    Tensor4D outputCache, targetCache;\n",
    "    DataType forward(const Tensor4D& output, const Tensor4D& target) {\n",
    "        Timer timer; timer.start();\n",
    "        outputCache.copyFrom(output); targetCache.copyFrom(target);\n",
    "        DataType loss = 0;\n",
    "        for (size_t i = 0; i < output.data.size(); i++) {\n",
    "            DataType diff = output.data[i] - target.data[i];\n",
    "            loss += diff * diff;\n",
    "        }\n",
    "        timer.stop(); gProfiler.addTime(\"MSELoss_forward\", timer.elapsedMs());\n",
    "        return loss / output.data.size();\n",
    "    }\n",
    "    Tensor4D backward() {\n",
    "        Timer timer; timer.start();\n",
    "        Tensor4D grad(outputCache.batch, outputCache.height, outputCache.width, outputCache.channels);\n",
    "        DataType scale = 2.0f / outputCache.data.size();\n",
    "        for (size_t i = 0; i < outputCache.data.size(); i++)\n",
    "            grad.data[i] = scale * (outputCache.data[i] - targetCache.data[i]);\n",
    "        timer.stop(); gProfiler.addTime(\"MSELoss_backward\", timer.elapsedMs());\n",
    "        return grad;\n",
    "    }\n",
    "};\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/autoencoder.h\n",
    "#ifndef AUTOENCODER_H\n",
    "#define AUTOENCODER_H\n",
    "\n",
    "#include \"common.h\"\n",
    "#include \"layers_cpu.h\"\n",
    "#include \"optimizer.h\"\n",
    "\n",
    "class Autoencoder {\n",
    "public:\n",
    "    Conv2D enc_conv1{3, 256, 3, 1, 1}, enc_conv2{256, 128, 3, 1, 1};\n",
    "    ReLU enc_relu1, enc_relu2;\n",
    "    MaxPool2D enc_pool1{2, 2}, enc_pool2{2, 2};\n",
    "    Conv2D dec_conv1{128, 128, 3, 1, 1}, dec_conv2{128, 256, 3, 1, 1}, dec_conv3{256, 3, 3, 1, 1};\n",
    "    ReLU dec_relu1, dec_relu2;\n",
    "    UpSample2D dec_up1{2}, dec_up2{2};\n",
    "    MSELoss lossFn;\n",
    "    Tensor4D dec_conv3_out;\n",
    "\n",
    "    Autoencoder() {\n",
    "        std::cout << \"Autoencoder initialized!\\n\";\n",
    "        std::cout << \"Architecture: Input(32,32,3) -> Conv(256) -> Pool -> Conv(128) -> Pool -> LATENT(8,8,128)\\n\";\n",
    "        std::cout << \"           -> Conv(128) -> Up -> Conv(256) -> Up -> Conv(3) -> Output(32,32,3)\\n\";\n",
    "        std::cout << \"Total params: ~751,875\\n\";\n",
    "    }\n",
    "\n",
    "    Tensor4D forward(const Tensor4D& input) {\n",
    "        auto x = enc_relu1.forward(enc_conv1.forward(input));\n",
    "        x = enc_pool1.forward(x);\n",
    "        x = enc_relu2.forward(enc_conv2.forward(x));\n",
    "        x = enc_pool2.forward(x);\n",
    "        x = dec_relu1.forward(dec_conv1.forward(x));\n",
    "        x = dec_up1.forward(x);\n",
    "        x = dec_relu2.forward(dec_conv2.forward(x));\n",
    "        x = dec_up2.forward(x);\n",
    "        dec_conv3_out = dec_conv3.forward(x);\n",
    "        return dec_conv3_out;\n",
    "    }\n",
    "\n",
    "    DataType backward(const Tensor4D& target) {\n",
    "        DataType loss = lossFn.forward(dec_conv3_out, target);\n",
    "        auto grad = lossFn.backward();\n",
    "        grad = dec_conv3.backward(grad);\n",
    "        grad = dec_up2.backward(grad); grad = dec_relu2.backward(grad); grad = dec_conv2.backward(grad);\n",
    "        grad = dec_up1.backward(grad); grad = dec_relu1.backward(grad); grad = dec_conv1.backward(grad);\n",
    "        grad = enc_pool2.backward(grad); grad = enc_relu2.backward(grad); grad = enc_conv2.backward(grad);\n",
    "        grad = enc_pool1.backward(grad); grad = enc_relu1.backward(grad); enc_conv1.backward(grad);\n",
    "        return loss;\n",
    "    }\n",
    "\n",
    "    void updateWeights(float lr) {\n",
    "        enc_conv1.updateWeights(lr); enc_conv2.updateWeights(lr);\n",
    "        dec_conv1.updateWeights(lr); dec_conv2.updateWeights(lr); dec_conv3.updateWeights(lr);\n",
    "    }\n",
    "\n",
    "    void updateWeightsAdam(AdamOptimizer& opt) {\n",
    "        opt.update(enc_conv1.weights.data, enc_conv1.gradWeights.data, 0);\n",
    "        opt.update(enc_conv1.bias, enc_conv1.gradBias, 1);\n",
    "        opt.update(enc_conv2.weights.data, enc_conv2.gradWeights.data, 2);\n",
    "        opt.update(enc_conv2.bias, enc_conv2.gradBias, 3);\n",
    "        opt.update(dec_conv1.weights.data, dec_conv1.gradWeights.data, 4);\n",
    "        opt.update(dec_conv1.bias, dec_conv1.gradBias, 5);\n",
    "        opt.update(dec_conv2.weights.data, dec_conv2.gradWeights.data, 6);\n",
    "        opt.update(dec_conv2.bias, dec_conv2.gradBias, 7);\n",
    "        opt.update(dec_conv3.weights.data, dec_conv3.gradWeights.data, 8);\n",
    "        opt.update(dec_conv3.bias, dec_conv3.gradBias, 9);\n",
    "    }\n",
    "};\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/main.cpp\n",
    "#include \"common.h\"\n",
    "#include \"cifar10.h\"\n",
    "#include \"autoencoder.h\"\n",
    "\n",
    "Profiler gProfiler;\n",
    "MemoryTracker gMemoryTracker;\n",
    "\n",
    "int main(int argc, char* argv[]) {\n",
    "    std::cout << \"=== AUTOENCODER CPU BASELINE ===\\n\";\n",
    "    \n",
    "    std::string dataPath = argc > 1 ? argv[1] : \"./cifar-10-batches-bin\";\n",
    "    int epochs = argc > 2 ? std::stoi(argv[2]) : 3;\n",
    "    int batchSize = argc > 3 ? std::stoi(argv[3]) : 32;\n",
    "    int maxSamples = argc > 4 ? std::stoi(argv[4]) : 0;\n",
    "    bool useAdam = argc > 5 ? (std::string(argv[5]) != \"sgd\") : true;\n",
    "\n",
    "    std::cout << \"Config: epochs=\" << epochs << \", batch=\" << batchSize \n",
    "              << \", samples=\" << (maxSamples > 0 ? std::to_string(maxSamples) : \"ALL\")\n",
    "              << \", optimizer=\" << (useAdam ? \"Adam\" : \"SGD\") << \"\\n\\n\";\n",
    "\n",
    "    CIFAR10Dataset dataset;\n",
    "    if (!dataset.load(dataPath, maxSamples)) return 1;\n",
    "\n",
    "    Autoencoder model;\n",
    "    AdamOptimizer adamOpt(0.001f);\n",
    "    \n",
    "    int numImages = dataset.getNumTrainImages();\n",
    "    int numBatches = (numImages + batchSize - 1) / batchSize;\n",
    "    \n",
    "    Timer totalTimer; totalTimer.start();\n",
    "    \n",
    "    for (int epoch = 0; epoch < epochs; epoch++) {\n",
    "        Timer epochTimer; epochTimer.start();\n",
    "        dataset.shuffleTrainData();\n",
    "        double epochLoss = 0;\n",
    "        \n",
    "        for (int b = 0; b < numBatches; b++) {\n",
    "            int start = b * batchSize;\n",
    "            int currBatch = std::min(batchSize, numImages - start);\n",
    "            \n",
    "            auto batch = dataset.getTrainBatch(start, currBatch);\n",
    "            model.forward(batch);\n",
    "            epochLoss += model.backward(batch);\n",
    "            \n",
    "            if (useAdam) { adamOpt.step(); model.updateWeightsAdam(adamOpt); }\n",
    "            else model.updateWeights(0.001f);\n",
    "        }\n",
    "        \n",
    "        epochTimer.stop();\n",
    "        std::cout << \"Epoch \" << (epoch+1) << \"/\" << epochs \n",
    "                  << \" | Loss: \" << std::fixed << std::setprecision(6) << (epochLoss/numBatches)\n",
    "                  << \" | Time: \" << std::setprecision(2) << epochTimer.elapsedSec() << \"s\\n\";\n",
    "    }\n",
    "    \n",
    "    totalTimer.stop();\n",
    "    std::cout << \"\\nTotal training time: \" << totalTimer.elapsedSec() << \" seconds\\n\";\n",
    "    \n",
    "    gProfiler.printReport();\n",
    "    gMemoryTracker.printReport();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a1687",
   "metadata": {},
   "source": [
    "## 2. Download CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abccf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download và extract CIFAR-10\n",
    "!wget -q https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "!tar -xzf cifar-10-binary.tar.gz\n",
    "!ls cifar-10-batches-bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeebfd3",
   "metadata": {},
   "source": [
    "## 3. Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile với optimization\n",
    "!g++ -std=c++17 -O3 -march=native -o autoencoder_cpu src/main.cpp\n",
    "print(\"✅ Compilation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c040055",
   "metadata": {},
   "source": [
    "## 4. Run Training\n",
    "\n",
    "Cú pháp: `./autoencoder_cpu <data_path> [epochs] [batch_size] [max_samples] [optimizer]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test nhanh với 100 samples, 2 epochs, Adam optimizer\n",
    "!./autoencoder_cpu ./cifar-10-batches-bin 2 32 100 adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test với SGD optimizer để so sánh\n",
    "!./autoencoder_cpu ./cifar-10-batches-bin 2 32 100 sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207209d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train với nhiều samples hơn (500 samples, 3 epochs)\n",
    "!./autoencoder_cpu ./cifar-10-batches-bin 3 32 500 adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ FULL DATASET - Rất lâu trên CPU! (chỉ chạy khi cần)\n",
    "# Uncomment dòng dưới để chạy full dataset\n",
    "# !./autoencoder_cpu ./cifar-10-batches-bin 5 32 0 adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a60702",
   "metadata": {},
   "source": [
    "## 5. Phân tích kết quả\n",
    "\n",
    "Sau khi chạy, bạn sẽ thấy:\n",
    "\n",
    "### Profiler Report:\n",
    "- **Conv2D_forward** và **Conv2D_backward** chiếm ~99% thời gian\n",
    "- Đây là bottleneck cần tối ưu trong Phase 2 (GPU)\n",
    "\n",
    "### Memory Report:\n",
    "- Weights: ~3 MB\n",
    "- Activations: Phụ thuộc batch size\n",
    "\n",
    "### Kết luận cho GPU Optimization:\n",
    "1. **Ưu tiên 1**: Conv2D operations (>99% time)\n",
    "2. **Kỹ thuật**: im2col + GEMM, shared memory tiling\n",
    "3. **Target speedup**: >20x so với CPU"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
